% $Log: abstract.tex,v $
% Revision 1.1  93/05/14  14:56:25  starflt
% Initial revision
% 
% Revision 1.1  90/05/04  10:41:01  lwvanels
% Initial revision
% 
%
%% The text of your abstract and nothing else (other than comments) goes here.
%% It will be single-spaced and the rest of the text that is supposed to go on
%% the abstract page will be generated by the abstractpage environment.  This
%% file should be \input (not \include 'd) from cover.tex.
Recent years have seen a surge of interest in non-parametric
reinforcement learning.
There are now practical non-parametric algorithms that use kernel regression to
approximate value functions.
The correctness guarantees of kernel regression require that the underlying
value function be smooth.
Most problems of interest do not satisfy this requirement in their native
space, but can be represented in such a way that they do.

In this thesis, we show that the ideal representation is one that maps points
directly to their values.
Existing representation discovery algorithms that have been used in parametric
reinforcement learning settings do not, in general, produce such a
representation.
We go on to present Fit-Improving Iterative Representation Adjustment
(FIIRA), a novel framework for function approximation and representation
discovery, which interleaves steps of value estimation and representation
adjustment to increase the expressive power of a given regression scheme.
We then show that FIIRA creates representations that correlate highly with
value, giving kernel regression the power to represent discontinuous functions.
Finally, we extend kernel-based reinforcement learning to use FIIRA and show
that this results in performance improvements on three
benchmark problems: Mountain-Car, Acrobot, and PinBall.
