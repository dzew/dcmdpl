%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Introduction}
The central goal of AI is to create systems that
display facets of human intelligence.
One such facet is the ability to learn from experience.
Reinforcement learning is an artificial intelligence paradigm for learning
how to behave from experience.
On some tasks, reinforcement learning algorithms have been able to produce
solutions that outperform those generated by humans.
But to generate these solutions, the computer invariably needs to collect
disproportionately more experience than the human learner.

One reason humans are able to learn from a small amount of experience is
that they can abstract away irrelevant details and focus on what matters.
Consider, for instance, a child trying different foods for the first time.
The child will quickly learn to visually distinguish between jalepe\~{n}os and
bell peppers after consuming a few of each.
That same child may go through life without ever learning that oranges and
tangerines are not the same thing.
This is despite the fact that tangerines are as visually distinct
from oranges as jalepe\~{n}os are from bell peppers.

The child makes one distinction but not the other because it
builds a representation of the world based on utility.
Oranges and tangerines have near identical utilities;
they are both sweet, citrus fruits that go well with breakfast.
There is no need to expend mental effort telling them apart.
On the other hand, bell peppers and jalepe\~{n}os have starkly different
utilities:
one is an innocuous vegetable that can be eaten at any time; the other
is a spicy fireball that must be handled with care.
Confusing the two can have dire consequences and so it is worth
learning the difference.

Computers are not self-driven to create such abstractions;
they must be programmed to do so.
Existing algorithms for learning abstractions do not explicitly
consider value in the process, which is analogous to classifying foods
by sight without ever tasting them;
it cannot result in an abstraction grounded in utility.
In this thesis I propose Fit-Improving Iterative Representation Adjustment
(FIIRA), a novel framework for function approximation and representation
discovery. FIIRA interleaves steps of value estimation (tasting the food) and
representation updates (looking at the food) in an attempt to create an
abstraction consistent with utility.

The remainder of this thesis is organized as follows.
Chapter 2 provides the reinforcement learning background necessary for
placing this thesis in context.
Chapter 3 discusses how and when changing representations can simplify a
reinforcement learning problem.
Chapter 4 introduces FIIRA and discusses its properties.
Chapter 5 demonstrates the performance of FIIRA on the benchmark
reinforcement learning problems Mountain-Car, Acrobot, and PinBall.
Finally, Chapter 6 concludes and outlines possible directions for future work.